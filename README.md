# TTG - Distributed Computation on Kubernetes

[![Status](https://img.shields.io/badge/status-milestone_1_complete-green.svg)]()
[![Version](https://img.shields.io/badge/version-1.1.0-blue.svg)]()
[![Kubernetes](https://img.shields.io/badge/kubernetes-1.27+-blue.svg)]()
[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)]()

A framework for running distributed computation workloads across a Kubernetes cluster. Designed to accelerate parameter-heavy algorithms by distributing work across multiple worker nodes.

> **üìä Latest Test:** 10,000 parameters processed across 3 workers in ~10 seconds

## üöÄ Quick Start (5 minutes)

### Prerequisites

- Docker installed and running
- `kubectl` installed
- `kind` installed (for local testing)

### 1. Create the Cluster

```bash
cd /home/xavierand_/Desktop/TTG

# Create local Kubernetes cluster with 3 worker nodes
./k8s/local/setup-local.sh
```

### 2. Build the Worker Image

```bash
# Build with version and load into kind cluster
./scripts/build.sh --version 1.1.0 --load-kind
```

### 3. Deploy Workers

```bash
kubectl apply -f k8s/manifests/parallel-jobs.yaml
```

### 4. Watch the Magic! ‚ú®

```bash
# Watch pods being created and running
kubectl get pods -l app.kubernetes.io/name=ttg-worker -w

# See which node each pod runs on
kubectl get pods -l app.kubernetes.io/name=ttg-worker -o wide

# View logs from all workers (with enhanced logging)
kubectl logs -l app.kubernetes.io/name=ttg-worker -f
```

### 5. View Resources

```bash
# See all TTG resources (Docker + Kubernetes)
./scripts/list-resources.sh
```

### 6. Clean Up

```bash
# Preview what will be deleted
./scripts/cleanup-all.sh --dry-run

# Delete job only (keep cluster for next run)
./scripts/cleanup-all.sh --keep-cluster

# Delete everything including cluster
./scripts/cleanup-all.sh --force
```

---

## üìñ Documentation

| Document                                                     | Description                               |
| ------------------------------------------------------------ | ----------------------------------------- |
| [SUPERVISOR_REPORT.md](SUPERVISOR_REPORT.md)                 | **Executive summary & quick start guide** |
| [docs/KIND_EXPLAINED.md](docs/KIND_EXPLAINED.md)             | Kind tutorial for Kubernetes beginners    |
| [docs/KUBERNETES_EXPLAINED.md](docs/KUBERNETES_EXPLAINED.md) | K8s concepts explained                    |
| [docs/CONFIGURATION_GUIDE.md](docs/CONFIGURATION_GUIDE.md)   | Configuration reference                   |
| [docs/TEST_RESULTS_v1.1.0.md](docs/TEST_RESULTS_v1.1.0.md)   | Detailed v1.1.0 test results              |
| [docs/PROJECT_OVERVIEW.md](docs/PROJECT_OVERVIEW.md)         | Project background & architecture         |
| [docs/KUBERNETES_SETUP.md](docs/KUBERNETES_SETUP.md)         | Setup instructions (local & Azure)        |

---

## üèóÔ∏è Project Structure

```
TTG/
‚îú‚îÄ‚îÄ src/                          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ worker.py                 # Main distributed worker (v1.1.0)
‚îÇ   ‚îî‚îÄ‚îÄ logging_config.py         # Structured logging infrastructure
‚îÇ
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile                # Multi-stage build with OCI labels
‚îÇ
‚îú‚îÄ‚îÄ k8s/
‚îÇ   ‚îú‚îÄ‚îÄ manifests/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ parallel-jobs.yaml    # Main deployment manifest
‚îÇ   ‚îú‚îÄ‚îÄ local/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kind-config.yaml      # Kind cluster configuration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ setup-local.sh        # Cluster setup script
‚îÇ   ‚îî‚îÄ‚îÄ azure/
‚îÇ       ‚îî‚îÄ‚îÄ setup-aks.sh          # Azure AKS setup (future)
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ build.sh                  # Versioned image building
‚îÇ   ‚îú‚îÄ‚îÄ list-resources.sh         # Resource inventory
‚îÇ   ‚îú‚îÄ‚îÄ cleanup-all.sh            # Comprehensive cleanup
‚îÇ   ‚îú‚îÄ‚îÄ deploy.sh                 # Deployment helper
‚îÇ   ‚îî‚îÄ‚îÄ run-local.sh              # Local testing
‚îÇ
‚îú‚îÄ‚îÄ docs/                         # Documentation
‚îú‚îÄ‚îÄ SUPERVISOR_REPORT.md          # Executive summary
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îî‚îÄ‚îÄ README.md                     # This file
```

---

## üéØ How It Works

### The Problem

You have **10 million parameters** to calculate. Running sequentially takes forever.

### The Solution

Split the work across multiple Kubernetes workers:

```
Worker 0: Parameters 0 - 3,333,333
Worker 1: Parameters 3,333,334 - 6,666,666
Worker 2: Parameters 6,666,667 - 9,999,999
```

Each worker runs in a container on a different node, processing in parallel.

### The Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    KUBERNETES CLUSTER                        ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ  Worker 0   ‚îÇ  ‚îÇ  Worker 1   ‚îÇ  ‚îÇ  Worker 2   ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  Node 1     ‚îÇ  ‚îÇ  Node 2     ‚îÇ  ‚îÇ  Node 3     ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ Params:     ‚îÇ  ‚îÇ Params:     ‚îÇ  ‚îÇ Params:     ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ 0-3.3M      ‚îÇ  ‚îÇ 3.3M-6.6M   ‚îÇ  ‚îÇ 6.6M-10M    ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚öôÔ∏è Configuration

### Environment Variables

| Variable           | Default | Description                       |
| ------------------ | ------- | --------------------------------- |
| `WORKER_ID`        | 0       | Unique worker identifier          |
| `TOTAL_WORKERS`    | 3       | Total number of workers           |
| `TOTAL_PARAMETERS` | 10000   | Total parameters to process       |
| `BATCH_SIZE`       | 500     | Parameters per progress update    |
| `SIMULATE_WORK_MS` | 1       | Simulated work time per parameter |
| `LOG_LEVEL`        | INFO    | DEBUG, INFO, WARNING, ERROR       |
| `LOG_FORMAT`       | text    | text (human) or json (machine)    |

### Kubernetes Resources

Workers are configured with resource limits to prevent starving other applications:

```yaml
resources:
  requests:
    cpu: "100m" # 0.1 CPU requested
    memory: "128Mi" # 128MB RAM requested
  limits:
    cpu: "500m" # Max 0.5 CPU
    memory: "256Mi" # Max 256MB RAM
```

---

## üîß Customizing the Worker

The worker (`src/worker.py`) contains a placeholder computation. Replace the `_compute_parameter` method with your actual algorithm:

```python
def _compute_parameter(self, param_id: int) -> Dict[str, Any]:
    """
    Process a single parameter and return the result.

    REPLACE THIS WITH YOUR ACTUAL ALGORITHM
    """
    # Your computation here
    result = your_algorithm(param_id)

    return {
        'param_id': param_id,
        'result': result,
        'worker_id': self.worker_id
    }
```

---

## ‚òÅÔ∏è Azure Deployment

For production-like testing on Azure AKS:

```bash
# Setup AKS cluster
chmod +x k8s/azure/setup-aks.sh
./k8s/azure/setup-aks.sh

# Build and push to Azure Container Registry
az acr login --name <your-acr-name>
docker build -t <your-acr-name>.azurecr.io/ttg-worker:latest -f docker/Dockerfile .
docker push <your-acr-name>.azurecr.io/ttg-worker:latest

# Update image reference in manifest and deploy
# (edit k8s/manifests/parallel-jobs.yaml)
kubectl apply -f k8s/manifests/parallel-jobs.yaml
```

See [docs/KUBERNETES_SETUP.md](docs/KUBERNETES_SETUP.md) for detailed Azure instructions.

---

## üìä Future Enhancements (Post Milestone 1)

- [ ] **Message Queue**: Add Redis/RabbitMQ for dynamic work distribution
- [ ] **Checkpointing**: Save progress for fault tolerance
- [ ] **Result Aggregation**: Collect and combine worker outputs
- [ ] **Observability**: Add Prometheus metrics and Grafana dashboards
- [ ] **Auto-scaling**: Scale workers based on queue depth
- [ ] **Persistent Storage**: Mount volumes for result persistence

---

## üêõ Troubleshooting

### Pods stuck in "Pending"

```bash
kubectl describe pod <pod-name>
# Check Events section for issues
```

Usually caused by insufficient resources. Check node capacity:

```bash
kubectl describe nodes | grep -A 10 "Allocated resources"
```

### Image not found

For kind (local):

```bash
kind load docker-image ttg-worker:latest --name ttg-sandbox
```

For AKS:

```bash
az acr login --name <acr-name>
docker push <acr-name>.azurecr.io/ttg-worker:latest
```

### View worker logs

```bash
# All workers
kubectl logs -l job-name=ttg-computation --all-containers

# Specific pod
kubectl logs <pod-name>

# Follow logs
kubectl logs -f <pod-name>
```

---

## üìù License

MIT License - See LICENSE file for details.

---

_Last Updated: 2026-01-27 | Version: 1.1.0 | Milestone 1: ‚úÖ Complete_
