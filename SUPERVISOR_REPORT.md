# TTG Distributed Computation System

## Supervisor Report - Milestone 1 Complete

**Report Date:** January 27, 2026  
**Version:** 1.1.0  
**Status:** âœ… **MILESTONE 1 COMPLETE**

---

## Executive Summary

We have successfully built and tested a **distributed computation system** that runs across multiple Kubernetes worker nodes. The system can process 10,000 parameters across 3 parallel workers in under 10 seconds, demonstrating linear scalability potential.

> **Note:** The current worker runs a **simulated workload** (1ms delay per parameter) to prove the infrastructure works. The real algorithm will be integrated in a future milestone.

### Key Achievements

| Metric                   | Result                             |
| ------------------------ | ---------------------------------- |
| **Workers**              | 3 parallel workers                 |
| **Nodes**                | 3 dedicated Kubernetes nodes       |
| **Processing Time**      | ~10 seconds (simulated workload)   |
| **Throughput**           | ~1,500 params/second (combined)    |
| **Parameters Processed** | 10,000                             |
| **Workload Type**        | âš ï¸ Simulated (1ms sleep per param) |
| **Test Status**          | âœ… All tests passed                |

---

## Quick Start Guide

### Prerequisites

- Docker Desktop (running)
- `kubectl` CLI installed
- `kind` CLI installed

### Step 1: Create the Cluster (One-time setup)

```bash
cd /home/xavierand_/Desktop/TTG
./k8s/local/setup-local.sh
```

This creates a local Kubernetes cluster with:

- 1 control plane node
- 3 worker nodes

**Expected output:** "âœ… Cluster ready!"

### Step 2: Build the Worker Image

```bash
./scripts/build.sh --version 1.1.0 --load-kind
```

This:

- Builds the Python worker Docker image
- Tags it as `ttg-worker:v1.1.0`
- Loads it into all kind cluster nodes

**Expected output:** "âœ“ Image loaded into kind cluster"

### Step 3: Deploy Workers

```bash
kubectl apply -f k8s/manifests/parallel-jobs.yaml
```

**Expected output:** "job.batch/ttg-computation created"

### Step 4: Watch Results

```bash
# Watch pods being created and completing
kubectl get pods -l app.kubernetes.io/name=ttg-worker -w

# View logs (live)
kubectl logs -l app.kubernetes.io/name=ttg-worker -f

# Check job status
kubectl get job ttg-computation
```

**Expected output:** All 3 pods show "Completed" status

### Step 5: Verify Distribution

```bash
# See which node each worker ran on
kubectl get pods -o wide
```

You should see each pod on a different node (worker, worker2, worker3).

### Step 6: View Resources

```bash
./scripts/list-resources.sh
```

Shows all TTG-related Docker and Kubernetes resources.

### Step 7: Clean Up

```bash
# Preview what will be deleted
./scripts/cleanup-all.sh --dry-run

# Actually delete everything
./scripts/cleanup-all.sh --force

# Or keep the cluster for next run
./scripts/cleanup-all.sh --keep-cluster
```

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         KIND KUBERNETES CLUSTER                          â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    CONTROL PLANE NODE                            â”‚    â”‚
â”‚  â”‚              (ttg-cluster-control-plane)                         â”‚    â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚    â”‚
â”‚  â”‚      â”‚  API Server  â”‚  Scheduler   â”‚  Controller  â”‚             â”‚    â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â”‚                                         â”‚
â”‚                                â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  WORKER NODE 1    â”‚  â”‚  WORKER NODE 2    â”‚  â”‚  WORKER NODE 3    â”‚   â”‚
â”‚  â”‚  (ttg-cluster-    â”‚  â”‚  (ttg-cluster-    â”‚  â”‚  (ttg-cluster-    â”‚   â”‚
â”‚  â”‚   worker)         â”‚  â”‚   worker2)        â”‚  â”‚   worker3)        â”‚   â”‚
â”‚  â”‚                   â”‚  â”‚                   â”‚  â”‚                   â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚ TTG Worker  â”‚  â”‚  â”‚  â”‚ TTG Worker  â”‚  â”‚  â”‚  â”‚ TTG Worker  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ Pod #0      â”‚  â”‚  â”‚  â”‚ Pod #1      â”‚  â”‚  â”‚  â”‚ Pod #2      â”‚  â”‚   â”‚
â”‚  â”‚  â”‚             â”‚  â”‚  â”‚  â”‚             â”‚  â”‚  â”‚  â”‚             â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ Params:     â”‚  â”‚  â”‚  â”‚ Params:     â”‚  â”‚  â”‚  â”‚ Params:     â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ 0-3332      â”‚  â”‚  â”‚  â”‚ 3333-6665   â”‚  â”‚  â”‚  â”‚ 6666-9999   â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How It Works

1. **Job Creation**: Kubernetes creates a Job with `completions: 3` and `parallelism: 3`
2. **Pod Scheduling**: The scheduler distributes pods across nodes using anti-affinity rules
3. **Work Division**: Each worker calculates its parameter range based on `WORKER_ID` and `TOTAL_WORKERS`
4. **Parallel Execution**: All 3 workers process their slice simultaneously
5. **Completion**: Kubernetes marks the job complete when all pods finish

---

## What Each Worker Computes (Current Implementation)

> âš ï¸ **Important:** The current computation is a **PLACEHOLDER/SIMULATION** to prove the infrastructure works. The real algorithm will be integrated in a future milestone.

### The Computation Code

**File:** `src/worker.py`  
**Method:** `_compute_parameter()` (lines 170-198)

```python
def _compute_parameter(self, param_id: int) -> Dict[str, Any]:
    """
    Process a single parameter and return the result.

    This is a PLACEHOLDER computation. Replace with your actual algorithm.
    """
    # 1. SIMULATED DELAY: Sleep for 1 millisecond per parameter
    if self.simulate_work_ms > 0:
        time.sleep(self.simulate_work_ms / 1000.0)  # Default: 1ms

    # 2. PLACEHOLDER COMPUTATION: Generate a hash and fake result
    input_string = f"param_{param_id}_worker_{self.worker_id}"
    hash_result = hashlib.sha256(input_string.encode()).hexdigest()[:16]

    # 3. FAKE NUMERICAL RESULT: Simple formula for demonstration
    numerical_result = (param_id * 7 + 13) % 1000 + float(f"0.{param_id % 100}")

    return {
        'param_id': param_id,
        'result': numerical_result,
        'hash': hash_result,
        'worker_id': self.worker_id,
        'timestamp': datetime.now(timezone.utc).isoformat()
    }
```

### Why It Takes ~10 Seconds

| Step                           | Calculation                   |
| ------------------------------ | ----------------------------- |
| Total parameters               | 10,000                        |
| Workers                        | 3 (parallel)                  |
| Parameters per worker          | ~3,333                        |
| Simulated delay per param      | 1 millisecond                 |
| Pure sleep time per worker     | 3,333 Ã— 1ms = **3.3 seconds** |
| Overhead (logging, hashing)    | ~3 seconds                    |
| **Total per worker**           | **~6.7 seconds**              |
| **Wall-clock time (parallel)** | **~10 seconds**               |

### What Milestone 1 Proves

| âœ… Proven                     | âŒ Not Yet Implemented |
| ----------------------------- | ---------------------- |
| Work distributes across nodes | Real algorithm         |
| Parallel execution works      | Result aggregation     |
| Kubernetes scheduling works   | Persistent storage     |
| Logging and monitoring works  | Error recovery         |
| Build/deploy pipeline works   | Auto-scaling           |

### To Replace With Real Algorithm

Edit `src/worker.py`, find the `_compute_parameter` method, and replace the placeholder:

```python
def _compute_parameter(self, param_id: int) -> Dict[str, Any]:
    # REMOVE: time.sleep simulation
    # REMOVE: hash placeholder

    # ADD YOUR REAL ALGORITHM HERE:
    result = your_actual_algorithm(param_id)

    return {
        'param_id': param_id,
        'result': result,
        'worker_id': self.worker_id,
        'timestamp': datetime.now(timezone.utc).isoformat()
    }
```

---

### Key Components

| Component    | File                               | Purpose                           |
| ------------ | ---------------------------------- | --------------------------------- |
| Worker Code  | `src/worker.py`                    | Python computation logic          |
| Logging      | `src/logging_config.py`            | Structured logging infrastructure |
| Docker Image | `docker/Dockerfile`                | Multi-stage build, OCI labels     |
| K8s Manifest | `k8s/manifests/parallel-jobs.yaml` | Parallel job definition           |
| Build Script | `scripts/build.sh`                 | Versioned image building          |
| Cleanup      | `scripts/cleanup-all.sh`           | Resource management               |

---

## Test Results Summary

### Latest Test Run: January 27, 2026

| Aspect           | Result  | Details                                          |
| ---------------- | ------- | ------------------------------------------------ |
| **Build**        | âœ… Pass | Image `ttg-worker:v1.1.0` built with OCI labels  |
| **Distribution** | âœ… Pass | Pod 0 â†’ worker2, Pod 1 â†’ worker, Pod 2 â†’ worker3 |
| **Execution**    | âœ… Pass | 3/3 completions in 10 seconds                    |
| **Logging**      | âœ… Pass | Structured logs with lifecycle events            |
| **Cleanup**      | âœ… Pass | Scripts work with dry-run mode                   |

### Worker Performance

| Worker | Node    | Params | Duration | Throughput |
| ------ | ------- | ------ | -------- | ---------- |
| 0      | worker2 | 3,333  | 6.67s    | 499 p/s    |
| 1      | worker  | 3,334  | 6.68s    | 499 p/s    |
| 2      | worker3 | 3,333  | 6.67s    | 499 p/s    |

**Total effective throughput: ~1,500 parameters/second**

### Sample Log Output

```
[2026-01-27 05:20:48.396] [INFO ] [WORKER-0] [worker] ğŸš€ LIFECYCLE: STARTING
[2026-01-27 05:20:48.396] [INFO ] [WORKER-0] [worker] âœ… LIFECYCLE: INITIALIZED
[2026-01-27 05:20:48.396] [INFO ] [WORKER-0] [worker] â–¶ï¸  LIFECYCLE: RUNNING - Processing 3333 items
[2026-01-27 05:20:49.397] [INFO ] [WORKER-0] [worker] ğŸ“Š LIFECYCLE: PROGRESS - 500/3333 (15.0%)
...
[2026-01-27 05:20:55.071] [INFO ] [WORKER-0] [worker] ğŸ‰ LIFECYCLE: COMPLETED - Duration: 6.67s
```

---

## Project Structure

```
TTG/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ worker.py                 # Main distributed worker
â”‚   â””â”€â”€ logging_config.py         # Logging infrastructure
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile                # Multi-stage build with OCI labels
â”‚
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ manifests/
â”‚   â”‚   â””â”€â”€ parallel-jobs.yaml    # Main deployment manifest
â”‚   â””â”€â”€ local/
â”‚       â”œâ”€â”€ kind-config.yaml      # Kind cluster configuration
â”‚       â””â”€â”€ setup-local.sh        # Cluster setup script
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build.sh                  # Versioned image building
â”‚   â”œâ”€â”€ list-resources.sh         # Resource inventory
â”‚   â””â”€â”€ cleanup-all.sh            # Comprehensive cleanup
â”‚
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ KIND_EXPLAINED.md         # Kind tutorial for beginners
â”‚   â”œâ”€â”€ KUBERNETES_EXPLAINED.md   # K8s concepts explained
â”‚   â”œâ”€â”€ CONFIGURATION_GUIDE.md    # Configuration reference
â”‚   â””â”€â”€ TEST_RESULTS_v1.1.0.md    # Detailed test results
â”‚
â”œâ”€â”€ README.md                     # Project readme
â””â”€â”€ SUPERVISOR_REPORT.md          # This document
```

### Files That Can Be Removed (Deprecated)

| File                 | Reason                                   | Action         |
| -------------------- | ---------------------------------------- | -------------- |
| `scripts/cleanup.sh` | Replaced by `cleanup-all.sh`             | Safe to delete |
| `src/utils.py`       | Not used (functions inline in worker.py) | Safe to delete |

---

## What Changed in v1.1.0

### Enhanced Logging

- Structured timestamps: `[YYYY-MM-DD HH:MM:SS.mmm]`
- Worker identification: `[WORKER-N]`
- Lifecycle events with emojis: ğŸš€ STARTING, âœ… INITIALIZED, â–¶ï¸ RUNNING, ğŸ“Š PROGRESS, ğŸ‰ COMPLETED
- JSON summary output for machine parsing
- Performance metrics (throughput, batch timing)

### Docker Improvements

- OCI-compliant image labels for traceability
- Version, build date, git commit embedded
- Custom `ttg.*` labels for filtering

### Kubernetes Improvements

- Standard `app.kubernetes.io/*` labels
- Custom `ttg.io/*` labels
- Monitoring annotations
- Enhanced pod naming

### New Scripts

- `build.sh` - Versioned builds with kind integration
- `list-resources.sh` - Complete resource inventory
- `cleanup-all.sh` - Safe cleanup with dry-run mode

---

## Scaling Considerations (Future Milestones)

### To Increase Workers

Edit `k8s/manifests/parallel-jobs.yaml`:

```yaml
spec:
  completions: 10 # Change from 3 to 10
  parallelism: 10 # Match completions
  # ...
  env:
    - name: TOTAL_WORKERS
      value: "10" # Must match completions
```

### To Increase Parameters

Edit the same file:

```yaml
env:
  - name: TOTAL_PARAMETERS
    value: "1000000" # 1 million parameters
```

### Estimated Scaling

| Workers | Parameters | Est. Time | Notes                        |
| ------- | ---------- | --------- | ---------------------------- |
| 3       | 10,000     | 10s       | Current (tested)             |
| 10      | 100,000    | ~30s      | Need 10 nodes                |
| 100     | 1,000,000  | ~30s      | Need 100 nodes (Azure AKS)   |
| 1,000   | 10,000,000 | ~30s      | Production scale (Azure AKS) |

---

## Next Steps

### âœ… Milestone 2: Message Queue Architecture (2-WEEK SPRINT)

> **Full details:** [docs/MILESTONE_2_MESSAGE_QUEUE.md](docs/MILESTONE_2_MESSAGE_QUEUE.md)

**Chosen Solution:** Redis Streams  
**Timeline:** 2 weeks (10 working days)  
**Target Completion:** February 13, 2026  
**Status:** ğŸš€ Ready to start

```
Week 1: Core Implementation          Week 2: Testing & Documentation
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Day 1-2: Redis + queue_utils.py      Day 6-7: Fault & Scale testing
Day 3-4: Worker integration          Day 8-9: Documentation update
Day 5:   Kind E2E test (10K)         Day 10:  Buffer / Demo
```

**Key Simplifications for 2-Week Delivery:**

- No separate Coordinator (Worker 0 initializes tasks)
- No separate Aggregator (query Redis manually)
- No Celery (pure Redis Streams)
- Feature toggle for safe rollback

**MVP Success Criteria:**

- [ ] Redis deployed in kind cluster
- [ ] Workers pull tasks dynamically from queue
- [ ] Fault tolerance proven (kill pod â†’ task reprocessed)
- [ ] 10K and 100K parameter tests pass
- [ ] Documentation updated

### Deferred to Future Phase

- [ ] Celery integration (2 weeks)
- [ ] Flower monitoring dashboard
- [ ] Persistent Redis storage
- [ ] Azure AKS deployment
- [ ] Real algorithm integration

---

## Contact & Support

**Documentation:**

- **Milestone 2 Plan:** [docs/MILESTONE_2_MESSAGE_QUEUE.md](docs/MILESTONE_2_MESSAGE_QUEUE.md) â¬…ï¸ NEW
- Kind guide: [docs/KIND_EXPLAINED.md](docs/KIND_EXPLAINED.md)
- K8s concepts: [docs/KUBERNETES_EXPLAINED.md](docs/KUBERNETES_EXPLAINED.md)
- Configuration: [docs/CONFIGURATION_GUIDE.md](docs/CONFIGURATION_GUIDE.md)

**Useful Commands:**

```bash
# Check cluster health
kubectl get nodes

# View all TTG resources
./scripts/list-resources.sh

# View logs in real-time
kubectl logs -l app.kubernetes.io/name=ttg-worker -f

# Get detailed pod info
kubectl describe pod <pod-name>
```

---

## ğŸ¬ Demo for Supervisor (Copy-Paste Commands)

### Quick Reference Card

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SUPERVISOR DEMO - QUICK COMMANDS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  FULL DEMO (from scratch):     ./scripts/demo-full.sh                      â”‚
â”‚                                  OR copy commands below                     â”‚
â”‚                                                                             â”‚
â”‚  RE-RUN DEMO (cluster exists): kubectl delete job ttg-computation          â”‚
â”‚                                kubectl apply -f k8s/manifests/parallel-... â”‚
â”‚                                                                             â”‚
â”‚  CLEANUP (TTG only):           ./scripts/cleanup-all.sh --keep-cluster     â”‚
â”‚  CLEANUP (everything):         ./scripts/cleanup-all.sh --force            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Option A: Full Demo from Scratch (~5 minutes)

Copy and paste these commands one by one:

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 1: Navigate to project
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
cd /home/xavierand_/Desktop/TTG

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 2: Create Kubernetes cluster (1-2 minutes)
# Creates: 1 control-plane + 3 worker nodes
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
./k8s/local/setup-local.sh

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 3: Build Docker image and load to cluster
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
./scripts/build.sh --version 1.1.0 --load-kind

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 4: Deploy the workers (3 parallel workers)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
kubectl apply -f k8s/manifests/parallel-jobs.yaml

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 5: Watch pods being created and completing (~10 seconds)
# Press Ctrl+C when all show "Completed"
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
kubectl get pods -l app.kubernetes.io/name=ttg-worker -w

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 6: Show distribution across nodes (THE KEY RESULT!)
# Each pod should be on a DIFFERENT node
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
kubectl get pods -o wide

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 7: View logs (shows processing output)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
kubectl logs -l app.kubernetes.io/name=ttg-worker --tail=20

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 8: Show job completion status
# Should show: COMPLETIONS 3/3
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
kubectl get job ttg-computation
```

---

### Option B: Quick Re-Demo (~2 minutes)

If cluster already exists from previous run:

```bash
cd /home/xavierand_/Desktop/TTG

# Delete previous job (if exists)
kubectl delete job ttg-computation --ignore-not-found=true

# Re-deploy workers
kubectl apply -f k8s/manifests/parallel-jobs.yaml

# Watch completion
kubectl get pods -l app.kubernetes.io/name=ttg-worker -w

# Show results
kubectl get pods -o wide
kubectl get job ttg-computation
```

---

### What to Show Supervisor

| Step                  | What to Point Out                                              |
| --------------------- | -------------------------------------------------------------- |
| **Pods starting**     | "3 pods starting simultaneously"                               |
| **Node distribution** | "Each pod runs on a different node (worker, worker2, worker3)" |
| **Completion**        | "All 3 completed in ~10 seconds"                               |
| **Logs**              | "Each worker processed ~3,333 parameters independently"        |

---

### ğŸ§¹ Cleanup Commands (SAFE - Only Deletes TTG Resources)

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OPTION 1: Delete job only (fastest, can re-demo immediately)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
kubectl delete job ttg-computation

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OPTION 2: Delete job + pods, keep cluster (good for re-demo later)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
./scripts/cleanup-all.sh --keep-cluster --force

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OPTION 3: Full cleanup - delete everything TTG-related
# (cluster, images, containers - BUT NOT other Docker resources)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
./scripts/cleanup-all.sh --force

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OPTION 4: Preview what will be deleted (dry-run, no actual deletion)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
./scripts/cleanup-all.sh --dry-run
```

**âš ï¸ SAFETY NOTE:** The cleanup script ONLY deletes TTG project resources:

- Kubernetes: Jobs/pods with label `app.kubernetes.io/name=ttg-worker`
- Kind cluster: `ttg-cluster` only
- Docker images: `ttg-worker:*` only

**Your other Docker containers, images, and resources are SAFE.**

---

## Appendix: Complete Command Reference

### Build Commands

```bash
# Standard build
./scripts/build.sh

# Build with specific version
./scripts/build.sh --version 1.1.0

# Build and load into kind
./scripts/build.sh --version 1.1.0 --load-kind
```

### Deploy Commands

```bash
# Deploy parallel job
kubectl apply -f k8s/manifests/parallel-jobs.yaml

# Delete job (to redeploy)
kubectl delete job ttg-computation

# Scale (edit manifest, then apply)
kubectl apply -f k8s/manifests/parallel-jobs.yaml
```

### Monitoring Commands

```bash
# Watch pods
kubectl get pods -w

# View logs (all workers)
kubectl logs -l app.kubernetes.io/name=ttg-worker

# View logs (specific worker)
kubectl logs ttg-computation-0-xxxxx

# Resource usage
kubectl top pods
```

### Cleanup Commands

```bash
# Preview cleanup
./scripts/cleanup-all.sh --dry-run

# Delete job only
kubectl delete job ttg-computation

# Full cleanup
./scripts/cleanup-all.sh --force

# Cleanup but keep cluster
./scripts/cleanup-all.sh --keep-cluster

# Delete entire cluster
kind delete cluster --name ttg-cluster
```

---

**Report Generated:** 2026-01-27 00:21 EST  
**Last Updated:** 2026-01-30  
**Version:** 1.1.0  
**Status:** Milestone 1 Complete âœ… | Milestone 2 Sprint Starting ğŸš€
