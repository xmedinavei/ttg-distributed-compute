# TTG - Distributed Computation on Kubernetes

## Project Status

| Item                   | Status             | Date       |
| ---------------------- | ------------------ | ---------- |
| Project Initialized    | âœ… Complete        | 2026-01-26 |
| Documentation          | âœ… Complete        | 2026-01-26 |
| Local K8s Setup (kind) | ğŸ”„ Ready to Deploy | 2026-01-26 |
| Worker Implementation  | âœ… Complete        | 2026-01-26 |
| Azure AKS Alternative  | ğŸ“‹ Documented      | 2026-01-26 |
| First Milestone        | ğŸ¯ In Progress     | -          |

---

## ğŸ“– Table of Contents

1. [Understanding the Problem](#understanding-the-problem)
2. [Why Kubernetes?](#why-kubernetes)
3. [Why a Sandbox Environment?](#why-a-sandbox-environment)
4. [Kubernetes Concepts Explained](#kubernetes-concepts-explained)
5. [Architecture Overview](#architecture-overview)
6. [Milestone 1 Requirements](#milestone-1-requirements)
7. [Decision Log](#decision-log)
8. [Challenges & Risks](#challenges--risks)

---

## ğŸ• Understanding the Problem

### The Pizza Analogy

Imagine you have a **giant pizza with 10 million slices** (10M parameters to calculate), and you need to eat all of them.

**The Sequential Approach (Current Problem):**

```
You alone eating 10M slices:
Slice 1 â†’ Slice 2 â†’ Slice 3 â†’ ... â†’ Slice 10,000,000
Time: FOREVER ğŸ˜±
```

**The Parallel Approach (Our Solution):**

```
You invite 10 friends (workers), each eats 1M slices:
Worker 1: Slices 1 - 1,000,000
Worker 2: Slices 1,000,001 - 2,000,000
Worker 3: Slices 2,000,001 - 3,000,000
...
Worker 10: Slices 9,000,001 - 10,000,000

Time: 10x faster! ğŸš€
```

### The Technical Reality

Your algorithm needs to process **10 million parameters**. Each parameter calculation is (presumably) independent, meaning:

- Parameter #1's result doesn't depend on Parameter #2's result
- They can be calculated **in parallel** by different machines
- Results are collected and aggregated at the end

**This is called "Embarrassingly Parallel" computation** - work that's easy to split because there are no dependencies between tasks.

### Visual Representation

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚          10 MILLION PARAMETERS              â”‚
                    â”‚  [1, 2, 3, 4, 5, ... 9,999,998, 9,999,999, 10,000,000]  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                          â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚    COORDINATOR    â”‚
                              â”‚  (splits work)    â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                           â–¼                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    WORKER 1     â”‚         â”‚    WORKER 2     â”‚         â”‚    WORKER 3     â”‚
    â”‚   Node: node-1  â”‚         â”‚   Node: node-2  â”‚         â”‚   Node: node-3  â”‚
    â”‚                 â”‚         â”‚                 â”‚         â”‚                 â”‚
    â”‚ Parameters:     â”‚         â”‚ Parameters:     â”‚         â”‚ Parameters:     â”‚
    â”‚ 1 - 3,333,333   â”‚         â”‚ 3,333,334 -     â”‚         â”‚ 6,666,668 -     â”‚
    â”‚                 â”‚         â”‚ 6,666,667       â”‚         â”‚ 10,000,000      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                           â”‚                           â”‚
              â–¼                           â–¼                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Partial Result â”‚         â”‚  Partial Result â”‚         â”‚  Partial Result â”‚
    â”‚      #1         â”‚         â”‚      #2         â”‚         â”‚      #3         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                           â”‚                           â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚    AGGREGATOR     â”‚
                              â”‚ (combines results)â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                          â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              FINAL RESULT                   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤” Why Kubernetes?

### What is Kubernetes (K8s)?

Kubernetes is a **container orchestration platform**. Think of it as a **smart manager** that:

1. **Schedules work**: Decides which machine runs which task
2. **Manages resources**: Ensures no machine is overloaded
3. **Handles failures**: Restarts crashed workers automatically
4. **Scales**: Easily add/remove workers as needed

### Why Not Just Use Multiple VMs or Docker Compose?

| Feature                  | VMs/Docker Compose | Kubernetes   |
| ------------------------ | ------------------ | ------------ |
| Auto-restart on failure  | âŒ Manual          | âœ… Automatic |
| Scale to 100s of workers | ğŸ˜“ Painful         | âœ… Easy      |
| Resource limits          | âŒ Manual config   | âœ… Built-in  |
| Load distribution        | âŒ Manual          | âœ… Automatic |
| Production-ready         | âŒ DIY             | âœ… Yes       |
| Learning curve           | Low                | Medium       |

### Kubernetes Value for This Project

```
Without K8s:
- You manually SSH into 10 machines
- You manually start workers on each
- If one crashes, you manually restart it
- If you need more workers, you manually provision machines

With K8s:
- Tell K8s: "Run 10 workers"
- K8s figures out where to run them
- K8s restarts crashed workers
- Tell K8s: "Now run 100 workers" â†’ Done
```

---

## ğŸ–ï¸ Why a Sandbox Environment?

### The Risk Without Sandbox

Your production Kubernetes cluster (`PROD k8s`) already runs critical applications. If you deploy your computation workload directly:

```
DANGER SCENARIO:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROD K8s CLUSTER                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  App A (critical)    App B (critical)    App C (critical)   â”‚
â”‚  CPU: 20%            CPU: 30%            CPU: 15%           â”‚
â”‚  Memory: 4GB         Memory: 8GB         Memory: 2GB        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  ğŸš¨ YOUR ALGORITHM DEPLOYS ğŸš¨                               â”‚
â”‚  Worker 1: CPU 100%, Memory 16GB                           â”‚
â”‚  Worker 2: CPU 100%, Memory 16GB                           â”‚
â”‚  Worker 3: CPU 100%, Memory 16GB                           â”‚
â”‚                                                             â”‚
â”‚  RESULT: App A, B, C CRASH! ğŸ’€                              â”‚
â”‚  (No resources left for them)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Sandbox Benefits

| Benefit             | Description                             |
| ------------------- | --------------------------------------- |
| **Isolation**       | Your tests don't affect production apps |
| **Experimentation** | Try different configs without fear      |
| **Cost Control**    | Know exact resources before production  |
| **Learning**        | Make mistakes safely                    |
| **Reproducibility** | Document setup for future use           |

### Sandbox Options

We have two options for sandbox:

1. **Local (kind/minikube)** - Recommended for learning
2. **Azure AKS** - For production-like testing

---

## ğŸ“š Kubernetes Concepts Explained

### For Absolute Beginners

#### 1. Cluster

A **cluster** is a group of machines (physical or virtual) working together.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KUBERNETES CLUSTER                    â”‚
â”‚                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  Machine 1  â”‚   â”‚  Machine 2  â”‚   â”‚  Machine 3  â”‚  â”‚
â”‚   â”‚   (Node)    â”‚   â”‚   (Node)    â”‚   â”‚   (Node)    â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. Node

A **node** is a single machine in the cluster. There are two types:

- **Control Plane Node**: The "brain" - schedules work, manages state
- **Worker Node**: The "hands" - actually runs your containers

```
Control Plane: "Hey Worker Node 2, run this container"
Worker Node 2: "On it, boss!" *runs container*
```

#### 3. Pod

A **Pod** is the smallest deployable unit. It's a wrapper around one or more containers.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            POD               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚      CONTAINER         â”‚  â”‚
â”‚  â”‚   (your Python app)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚
â”‚  Shared network, storage     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4. Job

A **Job** creates Pods to run a task to completion, then stops.

Perfect for our use case:

```
Job: "Process parameters 1-1000"
  â†’ Creates Pod
  â†’ Pod runs calculation
  â†’ Pod completes
  â†’ Job marks as "Completed"
```

#### 5. Deployment

A **Deployment** manages Pods that should run continuously (24/7 services).

```
Deployment: "Always keep 3 web servers running"
  â†’ If one crashes, automatically start a new one
```

#### 6. Service

A **Service** provides a stable network endpoint to access Pods.

```
Without Service:
  Pod IP: 10.0.0.5 (changes if Pod restarts!)

With Service:
  Service IP: 10.1.0.1 (stable)
  â†’ Routes to Pod 10.0.0.5
  â†’ If Pod restarts as 10.0.0.6, Service auto-updates
```

#### 7. ConfigMap & Secret

**ConfigMap**: Store configuration data (non-sensitive)
**Secret**: Store sensitive data (passwords, keys)

```yaml
# ConfigMap example
TOTAL_PARAMETERS: "10000000"
BATCH_SIZE: "1000"

# Secret example
DATABASE_PASSWORD: "***encrypted***"
```

### Quick Reference Table

| Concept    | Purpose                | Analogy              |
| ---------- | ---------------------- | -------------------- |
| Cluster    | Group of machines      | A company            |
| Node       | Single machine         | An employee          |
| Pod        | Container wrapper      | A desk               |
| Job        | Run-to-completion task | A one-time task      |
| Deployment | Long-running service   | A permanent position |
| Service    | Network endpoint       | Reception desk       |
| ConfigMap  | Configuration          | Employee handbook    |
| Secret     | Sensitive config       | Safe combination     |

---

## ğŸ—ï¸ Architecture Overview

### Phase 1: Simple Job-based Architecture (First Milestone)

For the first milestone, we use Kubernetes **Jobs** with **indexed completions**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     LOCAL K8s CLUSTER (kind)                    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    CONTROL PLANE                         â”‚   â”‚
â”‚  â”‚                    (kind-control-plane)                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                  â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚         â–¼                    â–¼                    â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   WORKER    â”‚      â”‚   WORKER    â”‚      â”‚   WORKER    â”‚    â”‚
â”‚  â”‚   NODE 1    â”‚      â”‚   NODE 2    â”‚      â”‚   NODE 3    â”‚    â”‚
â”‚  â”‚             â”‚      â”‚             â”‚      â”‚             â”‚    â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚ â”‚  Job-0  â”‚ â”‚      â”‚ â”‚  Job-1  â”‚ â”‚      â”‚ â”‚  Job-2  â”‚ â”‚    â”‚
â”‚  â”‚ â”‚ params: â”‚ â”‚      â”‚ â”‚ params: â”‚ â”‚      â”‚ â”‚ params: â”‚ â”‚    â”‚
â”‚  â”‚ â”‚ 0-3333  â”‚ â”‚      â”‚ â”‚ 3334-   â”‚ â”‚      â”‚ â”‚ 6667-   â”‚ â”‚    â”‚
â”‚  â”‚ â”‚         â”‚ â”‚      â”‚ â”‚ 6666    â”‚ â”‚      â”‚ â”‚ 9999    â”‚ â”‚    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 2: Queue-based Architecture (Future)

For more complex scenarios, use a message queue:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     K8s CLUSTER (future)                        â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚  COORDINATOR  â”‚                                              â”‚
â”‚  â”‚  (creates     â”‚                                              â”‚
â”‚  â”‚   tasks)      â”‚                                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚          â”‚ publishes tasks                                      â”‚
â”‚          â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚              MESSAGE QUEUE                    â”‚              â”‚
â”‚  â”‚     (Redis/RabbitMQ/Azure Service Bus)        â”‚              â”‚
â”‚  â”‚  [Task1][Task2][Task3][Task4][Task5]...       â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚          â”‚                                                      â”‚
â”‚          â”‚ workers pull tasks                                   â”‚
â”‚          â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Worker1 â”‚   â”‚ Worker2 â”‚   â”‚ Worker3 â”‚   â”‚ Worker4 â”‚  ...   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚          â”‚           â”‚             â”‚             â”‚              â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                              â”‚                                  â”‚
â”‚                              â–¼                                  â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                    â”‚   RESULT STORE  â”‚                          â”‚
â”‚                    â”‚ (Redis/Storage) â”‚                          â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Milestone 1 Requirements

### Goal

Setup sandbox Kubernetes environment with 3 worker nodes and run distributed computation.

### Success Criteria

| #   | Criterion             | Measurable Outcome                        |
| --- | --------------------- | ----------------------------------------- |
| 1   | K8s cluster running   | `kubectl get nodes` shows 3+ nodes        |
| 2   | Workers containerized | Docker image built successfully           |
| 3   | Jobs distributed      | Jobs run on different nodes               |
| 4   | Work partitioned      | Each job processes unique parameter range |
| 5   | Results visible       | `kubectl logs` shows computation output   |
| 6   | Documented            | All steps reproducible from docs          |

### Deliverables

1. âœ… Documentation (this file + setup guides)
2. âœ… Local K8s cluster configuration (kind)
3. âœ… Python worker implementation
4. âœ… Docker image for worker
5. âœ… Kubernetes Job manifests
6. âœ… Azure AKS alternative documentation

---

## ğŸ“ Decision Log

| Date       | Decision                         | Rationale                             | Alternatives Considered |
| ---------- | -------------------------------- | ------------------------------------- | ----------------------- |
| 2026-01-26 | Use kind for local K8s           | Faster setup, free, good for learning | minikube, k3s           |
| 2026-01-26 | Start with K8s Jobs (not queues) | Simpler for first milestone           | Celery, RabbitMQ, Kafka |
| 2026-01-26 | Python for workers               | User preference, good ecosystem       | Go, Java                |
| 2026-01-26 | 3 worker nodes                   | Matches milestone requirement         | More nodes              |
| 2026-01-26 | Indexed Jobs for distribution    | K8s native, no external dependencies  | Job arrays, manual      |

---

## âš ï¸ Challenges & Risks

### Current Challenges

| Challenge               | Impact | Mitigation                            |
| ----------------------- | ------ | ------------------------------------- |
| Learning curve (K8s)    | Medium | Detailed docs, simple first milestone |
| Resource limits unknown | Medium | Start small, profile, adjust          |
| No actual algorithm yet | Low    | Use placeholder computation           |

### Potential Risks

| Risk                               | Probability | Impact | Mitigation                 |
| ---------------------------------- | ----------- | ------ | -------------------------- |
| Local machine can't handle 3 nodes | Medium      | High   | Use lightweight config     |
| Docker not installed               | Low         | High   | Include installation steps |
| Network issues with kind           | Low         | Medium | Troubleshooting guide      |

---

## ğŸ“‚ Project Structure

```
TTG/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ PROJECT_OVERVIEW.md      # â† You are here
â”‚   â””â”€â”€ KUBERNETES_SETUP.md      # Setup instructions
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile               # Worker container
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ worker.py                # Computation worker
â”‚   â””â”€â”€ utils.py                 # Utilities
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ local/
â”‚   â”‚   â”œâ”€â”€ kind-config.yaml     # 3-node cluster config
â”‚   â”‚   â””â”€â”€ setup-local.sh       # Local setup script
â”‚   â”œâ”€â”€ azure/
â”‚   â”‚   â””â”€â”€ setup-aks.sh         # Azure AKS setup
â”‚   â””â”€â”€ manifests/
â”‚       â”œâ”€â”€ worker-job.yaml      # Single job template
â”‚       â””â”€â”€ parallel-jobs.yaml   # Parallel job spec
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build.sh                 # Build Docker image
â”‚   â”œâ”€â”€ deploy.sh                # Deploy to K8s
â”‚   â””â”€â”€ cleanup.sh               # Clean up resources
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # Quick start
```

---

## ğŸ”œ Next Steps

1. **Read** [KUBERNETES_SETUP.md](./KUBERNETES_SETUP.md) for setup instructions
2. **Choose** Local (kind) or Azure (AKS)
3. **Follow** the step-by-step guide
4. **Run** your first distributed computation!

---

## ğŸ“š Additional Resources

- [Kubernetes Official Docs](https://kubernetes.io/docs/home/)
- [kind Documentation](https://kind.sigs.k8s.io/)
- [Azure AKS Documentation](https://docs.microsoft.com/en-us/azure/aks/)
- [Docker Documentation](https://docs.docker.com/)

---

_Last Updated: 2026-01-26_
_Author: TTG Team_
